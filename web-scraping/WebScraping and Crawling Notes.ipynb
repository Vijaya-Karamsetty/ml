{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "    IMAGINE YOU HAVE TO PULL A LARGE AMOUNT OF DATA FROM WEBSITES AND YOU WANT TO DO IT AS QUICKLY AS POSSIBLE. HOW WOULD YOU DO IT WITHOUT MANUALLY GOING TO EACH WEBSITE AND GETTING THE DATA? WELL, “WEB SCRAPING” IS THE ANSWER. WEB SCRAPING JUST MAKES THIS JOB EASIER AND FASTER. \n",
    "    WEB SCRAPING IS A TERM USED TO DESCRIBE THE USE OF A PROGRAM OR ALGORITHM TO EXTRACT AND PROCESS LARGE AMOUNTS OF DATA FROM THE WEB. WHETHER YOU ARE A DATA SCIENTIST, ENGINEER, OR ANYBODY WHO ANALYZES LARGE AMOUNTS OF DATASETS, THE ABILITY TO SCRAPE DATA FROM THE WEB IS A USEFUL SKILL TO HAVE. LET'S SAY YOU FIND DATA FROM THE WEB, AND THERE IS NO DIRECT WAY TO DOWNLOAD IT, WEB SCRAPING USING PYTHON IS A SKILL YOU CAN USE TO EXTRACT THE DATA INTO A USEFUL FORM THAT CAN BE IMPORTED.\n",
    "    AS A PART OF THIS PROJECT, YOU WILL BE PREPARING YOUR OWN DATASETS ABOUT THE VARIOUS DAILY ESSENTIAL (OR MAY BE FAST MOVING CONSUMER GOOD FMCG) PRODUCTS SOLD ON VARIOUS ECOMMERCE PLATFORMS LIKE (BUT NOT LIMITED TO) BIGBASKET, GROFERS, RELIANCE SMART, DMART, AMAZON PANTRY ETC. THE MAIN OBJECTIVE OF THIS PROJECT IS TO GIVE YOU REAL LIFE EXPERIENCE WHILE DOING DATA ACQUISTION, DATA INTEGRATION, DATA CLEANING AND DATA TRANSFORAMTION BEFORE ATTEMPTING ANY OF THE ANALYTICAL ACTIVITY ON THE DATA.\n",
    "    THE FOUR TASKS THAT YOU WILL BE DOING AS A PART OF THIS EXERCISE WILL BE AS FOLLOWS:\n",
    "\n",
    "1. DATA ACQUISITION\n",
    "\n",
    "> YOU AS TEAM OF THREE OR FOUR STUDENTS HAS TO ACQUIRE THE DATA FROM THREE OR FOUR ECOMMERCE PLATFORMS USING THE TECHNIQUE OF WEB SCRAPPING. \n",
    "\n",
    ">FURTHER ON YOU CAN CONCENTRATE YOUR SEARCH BASED ON SOME OF THE CATEGORIES OF PRODUCTS LIKE VEGETABLES/FRUITS, GROCERY ITEMS, BRANDED ITEMS ETC. \n",
    "\n",
    ">OUTCOME OF THIS STEP WILL BE THREE OR FOUR DIFFERENT DATA SETS EACH ONE BELONGING TO ONE PLATFORM.\n",
    "\n",
    ">DOCUMENT ALL YOUR EFFORTS APPROPRIATELY IN THE JUPYTER NOTEBOOKS WITH DESCRIPTION AND CODE. \n",
    "\n",
    ">THE WEIGTAGE FOR THIS TASK WILL BE 15 MARKS\n",
    "\n",
    "2. DATA CLEANING\n",
    "\n",
    ">THE THREE OR FOUR DATASETS YOU HAVE GATHERED MIGHT HAVE SOME QUALITY ISSUES IN THEM LIKE (BUT NOT LIMITED TO) MISSING VALUES, DUPLICATE RECORDS, AND DERIVED ATTRIBUTES.\n",
    "\n",
    ">YOU HAVE TO CLEANSE YOUR DATASETS TO REMOVE ALL SUCH DAUNTING ISSSUES. \n",
    "\n",
    ">NARRATE ALL THE ISSUES WHICH YOU ENCOUNTER DURING THIS EXERCISE CLEARLY WITH APPROPRIATE EXPLANATION AND CODE. \n",
    "\n",
    ">THE WEIGTAGE FOR THIS TASK WILL BE 5 MARKS.\n",
    "\n",
    "3. DATA INTEGRATION\n",
    "\n",
    ">BY THIS TIME YOU MUST HAVE CLEANSED DATA AVAILABLE WITH YOU. FOR FURTHER PROCESSING YOU NEED TO INTEGRATE ALL THESE DATASETS TOGETHER INTO A SINGLE DATASET. \n",
    "\n",
    ">YOU MAY NEED TO DECIDE UPON A COMMON SCHEMA FOR THIS ACTIVITY WHICH CAN BE APPLIED ON THE ALL DATASETS. YOU MAY THINK OF ADDING OR DELETING OR MODIFYING THE ATTRIBUTES OF THE EXISTING DATASETS. \n",
    "\n",
    ">CAPTURE ALL THE THINKING GONE BEHIND PREPARING SUCH A SINGLE DATABASE IN THE DESCRIPTIVE MANNER IN THE JUPYTER NOTEBOOK ALONG WITH THE CODE. \n",
    "\n",
    ">THE WEIGTAGE FOR THIS TASK WILL BE 5 MARKS.\n",
    "\n",
    "4. EXPLORATORY DATA ANALYSIS AND RECOMMENDATION\n",
    "\n",
    ">USE THE EXPLORATORY DATA ANALYSIS TECHNIQUE ON THE DATASET IN ORDER TO FIND OUT THE INTERESTING INSIGHTS THAT ARE HIDDEN WITHIN THE DATA CAPTURED. \n",
    "\n",
    ">NOW ASSUME THAT A CUSTOMER IS LOOKING FOR A PARTICULAR ITEM, THEN WITH HELP OF SIMPLE PROGRAM YOU SHOULD BE ABLE TO RECOMMEND THE ONLINE PLATFORM FROM WHICH HE SHOULD MAKE A PURCHASE OF THIS ITEM. \n",
    "\n",
    ">DESCRIBE ALL EDA STEPS THOSE ARE DONE WITH THE OBSERVATIONS OBTAINED OUT OF IT WITH THE HELP OF PYTHON CODE IN JUPYTER NOTEBOOK. \n",
    "\n",
    ">THE WEIGTAGE FOR THIS TASK WILL BE 5 MARKS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "## Fast Moving Consumer Goods\n",
    "\n",
    "### What Are Fast-Moving Consumer Goods (FMCG)?\n",
    "\n",
    "**Fast-moving consumer goods** are products that sell quickly at relatively low cost. These goods are also called consumer packaged goods.\n",
    "\n",
    "**FMCG**s have a short shelf life because of high consumer demand (e.g., soft drinks and confections) or because they are perishable (e.g., meat, dairy products, and baked goods). These goods are purchased frequently, are consumed rapidly, are priced low, and are sold in large quantities. They also have a high turnover when they're on the shelf at the store.\n",
    "\n",
    "### Types of Fast-Moving Consumer Goods\n",
    "\n",
    "As mentioned above, fast-moving consumer goods are nondurable goods, or goods that have a short lifespan, and are consumed at a rapid or fast pace.\n",
    "\n",
    "__**FMCG**s can be divided into several different categories including:__\n",
    "\n",
    "    1. Processed foods: Cheese products, cereals, and boxed pasta\n",
    "    2. Prepared meals: Ready-to-eat meals\n",
    "    3. Beverages: Bottled water, energy drinks, and juices\n",
    "    4. Baked goods: Cookies, croissants, and bagels\n",
    "    5. Fresh, frozen foods, and dry goods: Fruits, vegetables, frozen peas and carrots, and raisins and nuts\n",
    "    6. Medicines: Aspirin, pain relievers, and other medication that can be purchased without a prescription\n",
    "    7. Cleaning products: Baking soda, oven cleaner, and window and glass cleaner\n",
    "    8. Cosmetics and toiletries: Hair care products, concealers, toothpaste, and soap\n",
    "    9. Office supplies: Pens, pencils, and markers\n",
    "#### References\n",
    ". investopedia [https://www.investopedia.com/terms/f/fastmoving-consumer-goods-fmcg.asp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "### Web Scraping\n",
    "\n",
    "If you’ve ever copy and pasted information from a website, you’ve performed the same function as any web scraper, only on a microscopic, manual scale.\n",
    "\n",
    "Web scraping, also known as web data extraction, is the process of retrieving or “scraping” data from a website. Unlike the mundane, mind-numbing process of manually extracting data, web scraping uses intelligent automation to retrieve hundreds, millions, or even billions of data points from the internet’s seemingly endless frontier.\n",
    "\n",
    "More than a modern convenience, the true power of web scraping lies in its ability to build and power some of the world’s most revolutionary business applications. ‘Transformative’ doesn’t even begin to describe the way some companies use web scraped data to enhance their operations, informing executive decisions all the way down to individual customer service experiences. \n",
    "\n",
    "### The basics of web scraping\n",
    "It’s extremely simple, in truth, and works by way of two parts: a web crawler and a web scraper. The web crawler is the horse, and the scraper is the chariot. The crawler leads the scraper, as if by the hand, through the internet, where it extracts the data requested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawling\n",
    "\n",
    "Website Crawling is the automated fetching of web pages by a software process, the purpose of which is to index the content of websites so they can be searched. The crawler analyzes the content of a page looking for links to the next pages to fetch and index.\n",
    "\n",
    "##### Types of Crawls\n",
    "There are mainly two types of crawling that get content from the web\n",
    "\n",
    "###### Site Crawl\n",
    "\n",
    "> is an attempt to crawl an entire site at one time, starting with the home page. It will grab links from that page, to continue crawling the site to other content of the site. This is often called “Spidering”.\n",
    "\n",
    "###### Page Crawl\n",
    "\n",
    "> which is an attempt by a crawler to crawl a single page or blog post for the content retrieval\n",
    "\n",
    "\n",
    "##### Crawler\n",
    "> A crawler is a software process that goes out to websites and requests the content as a browser would. After that, an indexing process actually picks out the content it wants to save. Typically the content that is indexed is any text visible on the page.\n",
    "\n",
    "##### /robots.txt\n",
    "\n",
    "    1. The site owner denies indexing and or crawling using a robots.txt file.\n",
    "    2. The page itself may indicate it’s not to be indexed and links not followed (directives embedded in the page code). These directives are “meta” tags that tell the crawler how it is allowed to interact with the site.\n",
    "    3. The site owner blocked a specific crawler IP address or “user agent”.\n",
    "For more details on the robots.txt please refer this link[http://www.robotstxt.org/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### installables\n",
    "> pip install webdriver-manager [https://pypi.org/project/webdriver-manager/]\n",
    "> conda install webdriver-manager\n",
    "\n",
    "> pip install selenium\n",
    "> conda install -c conda-forge selenium\n",
    "\n",
    "> pip install bs4\n",
    "> conda install -c anaconda beautifulsoup4\n",
    "\n",
    "> pip install urllib3\n",
    "> conda install urllib3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a Site URL to crawl and get the necessary indexes for scraping\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, url, support_dict):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping\n",
    "A web scraper is a specialized tool designed to accurately and quickly extract data from a web page. Web scrapers vary widely in design and complexity, depending on the project.\n",
    "\n",
    "The web scraping process: 3 simple steps\n",
    "\n",
    "1. First, our team of seasoned scraping veterans develops a scraper unique to your project, designed specifically to target and extract the data you want from the websites you want it from.\n",
    "2.\tThe data is retrieved in HTML format, after which it is carefully parsed to extricate the raw data you want from the noise surrounding it. Depending on the project, the data can be as simple as a name and address in some cases, and as complex as high dimensional weather and seed germination data the next.\n",
    "3.\tUltimately, the data is stored in the format and to the exact specifications of the project. Some companies use third party applications or databases to view and manipulate the data to their choosing, while others prefer it in a simple, raw format - generally as CSV, TSV or JSON."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
