{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "    IMAGINE YOU HAVE TO PULL A LARGE AMOUNT OF DATA FROM WEBSITES AND YOU WANT TO DO IT AS QUICKLY AS POSSIBLE. HOW WOULD YOU DO IT WITHOUT MANUALLY GOING TO EACH WEBSITE AND GETTING THE DATA? WELL, “WEB SCRAPING” IS THE ANSWER. WEB SCRAPING JUST MAKES THIS JOB EASIER AND FASTER. \n",
    "    WEB SCRAPING IS A TERM USED TO DESCRIBE THE USE OF A PROGRAM OR ALGORITHM TO EXTRACT AND PROCESS LARGE AMOUNTS OF DATA FROM THE WEB. WHETHER YOU ARE A DATA SCIENTIST, ENGINEER, OR ANYBODY WHO ANALYZES LARGE AMOUNTS OF DATASETS, THE ABILITY TO SCRAPE DATA FROM THE WEB IS A USEFUL SKILL TO HAVE. LET'S SAY YOU FIND DATA FROM THE WEB, AND THERE IS NO DIRECT WAY TO DOWNLOAD IT, WEB SCRAPING USING PYTHON IS A SKILL YOU CAN USE TO EXTRACT THE DATA INTO A USEFUL FORM THAT CAN BE IMPORTED.\n",
    "    AS A PART OF THIS PROJECT, YOU WILL BE PREPARING YOUR OWN DATASETS ABOUT THE VARIOUS DAILY ESSENTIAL (OR MAY BE FAST MOVING CONSUMER GOOD FMCG) PRODUCTS SOLD ON VARIOUS ECOMMERCE PLATFORMS LIKE (BUT NOT LIMITED TO) BIGBASKET, GROFERS, RELIANCE SMART, DMART, AMAZON PANTRY ETC. THE MAIN OBJECTIVE OF THIS PROJECT IS TO GIVE YOU REAL LIFE EXPERIENCE WHILE DOING DATA ACQUISTION, DATA INTEGRATION, DATA CLEANING AND DATA TRANSFORAMTION BEFORE ATTEMPTING ANY OF THE ANALYTICAL ACTIVITY ON THE DATA.\n",
    "    THE FOUR TASKS THAT YOU WILL BE DOING AS A PART OF THIS EXERCISE WILL BE AS FOLLOWS:\n",
    "\n",
    "1. DATA ACQUISITION\n",
    "\n",
    "> YOU AS TEAM OF THREE OR FOUR STUDENTS HAS TO ACQUIRE THE DATA FROM THREE OR FOUR ECOMMERCE PLATFORMS USING THE TECHNIQUE OF WEB SCRAPPING. \n",
    "\n",
    ">FURTHER ON YOU CAN CONCENTRATE YOUR SEARCH BASED ON SOME OF THE CATEGORIES OF PRODUCTS LIKE VEGETABLES/FRUITS, GROCERY ITEMS, BRANDED ITEMS ETC. \n",
    "\n",
    ">OUTCOME OF THIS STEP WILL BE THREE OR FOUR DIFFERENT DATA SETS EACH ONE BELONGING TO ONE PLATFORM.\n",
    "\n",
    ">DOCUMENT ALL YOUR EFFORTS APPROPRIATELY IN THE JUPYTER NOTEBOOKS WITH DESCRIPTION AND CODE. \n",
    "\n",
    ">THE WEIGTAGE FOR THIS TASK WILL BE 15 MARKS\n",
    "\n",
    "2. DATA CLEANING\n",
    "\n",
    ">THE THREE OR FOUR DATASETS YOU HAVE GATHERED MIGHT HAVE SOME QUALITY ISSUES IN THEM LIKE (BUT NOT LIMITED TO) MISSING VALUES, DUPLICATE RECORDS, AND DERIVED ATTRIBUTES.\n",
    "\n",
    ">YOU HAVE TO CLEANSE YOUR DATASETS TO REMOVE ALL SUCH DAUNTING ISSSUES. \n",
    "\n",
    ">NARRATE ALL THE ISSUES WHICH YOU ENCOUNTER DURING THIS EXERCISE CLEARLY WITH APPROPRIATE EXPLANATION AND CODE. \n",
    "\n",
    ">THE WEIGTAGE FOR THIS TASK WILL BE 5 MARKS.\n",
    "\n",
    "3. DATA INTEGRATION\n",
    "\n",
    ">BY THIS TIME YOU MUST HAVE CLEANSED DATA AVAILABLE WITH YOU. FOR FURTHER PROCESSING YOU NEED TO INTEGRATE ALL THESE DATASETS TOGETHER INTO A SINGLE DATASET. \n",
    "\n",
    ">YOU MAY NEED TO DECIDE UPON A COMMON SCHEMA FOR THIS ACTIVITY WHICH CAN BE APPLIED ON THE ALL DATASETS. YOU MAY THINK OF ADDING OR DELETING OR MODIFYING THE ATTRIBUTES OF THE EXISTING DATASETS. \n",
    "\n",
    ">CAPTURE ALL THE THINKING GONE BEHIND PREPARING SUCH A SINGLE DATABASE IN THE DESCRIPTIVE MANNER IN THE JUPYTER NOTEBOOK ALONG WITH THE CODE. \n",
    "\n",
    ">THE WEIGTAGE FOR THIS TASK WILL BE 5 MARKS.\n",
    "\n",
    "4. EXPLORATORY DATA ANALYSIS AND RECOMMENDATION\n",
    "\n",
    ">USE THE EXPLORATORY DATA ANALYSIS TECHNIQUE ON THE DATASET IN ORDER TO FIND OUT THE INTERESTING INSIGHTS THAT ARE HIDDEN WITHIN THE DATA CAPTURED. \n",
    "\n",
    ">NOW ASSUME THAT A CUSTOMER IS LOOKING FOR A PARTICULAR ITEM, THEN WITH HELP OF SIMPLE PROGRAM YOU SHOULD BE ABLE TO RECOMMEND THE ONLINE PLATFORM FROM WHICH HE SHOULD MAKE A PURCHASE OF THIS ITEM. \n",
    "\n",
    ">DESCRIBE ALL EDA STEPS THOSE ARE DONE WITH THE OBSERVATIONS OBTAINED OUT OF IT WITH THE HELP OF PYTHON CODE IN JUPYTER NOTEBOOK. \n",
    "\n",
    ">THE WEIGTAGE FOR THIS TASK WILL BE 5 MARKS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "\n",
    "## Fast Moving Consumer Goods\n",
    "\n",
    "### What Are Fast-Moving Consumer Goods (FMCG)?\n",
    "\n",
    "**Fast-moving consumer goods** are products that sell quickly at relatively low cost. These goods are also called consumer packaged goods.\n",
    "\n",
    "**FMCG**s have a short shelf life because of high consumer demand (e.g., soft drinks and confections) or because they are perishable (e.g., meat, dairy products, and baked goods). These goods are purchased frequently, are consumed rapidly, are priced low, and are sold in large quantities. They also have a high turnover when they're on the shelf at the store.\n",
    "\n",
    "### Types of Fast-Moving Consumer Goods\n",
    "\n",
    "As mentioned above, fast-moving consumer goods are nondurable goods, or goods that have a short lifespan, and are consumed at a rapid or fast pace.\n",
    "\n",
    "__**FMCG**s can be divided into several different categories including:__\n",
    "\n",
    "    1. Processed foods: Cheese products, cereals, and boxed pasta\n",
    "    2. Prepared meals: Ready-to-eat meals\n",
    "    3. Beverages: Bottled water, energy drinks, and juices\n",
    "    4. Baked goods: Cookies, croissants, and bagels\n",
    "    5. Fresh, frozen foods, and dry goods: Fruits, vegetables, frozen peas and carrots, and raisins and nuts\n",
    "    6. Medicines: Aspirin, pain relievers, and other medication that can be purchased without a prescription\n",
    "    7. Cleaning products: Baking soda, oven cleaner, and window and glass cleaner\n",
    "    8. Cosmetics and toiletries: Hair care products, concealers, toothpaste, and soap\n",
    "    9. Office supplies: Pens, pencils, and markers\n",
    "#### References\n",
    ". investopedia [https://www.investopedia.com/terms/f/fastmoving-consumer-goods-fmcg.asp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "### Web Scraping\n",
    "\n",
    "If you’ve ever copy and pasted information from a website, you’ve performed the same function as any web scraper, only on a microscopic, manual scale.\n",
    "\n",
    "Web scraping, also known as web data extraction, is the process of retrieving or “scraping” data from a website. Unlike the mundane, mind-numbing process of manually extracting data, web scraping uses intelligent automation to retrieve hundreds, millions, or even billions of data points from the internet’s seemingly endless frontier.\n",
    "\n",
    "More than a modern convenience, the true power of web scraping lies in its ability to build and power some of the world’s most revolutionary business applications. ‘Transformative’ doesn’t even begin to describe the way some companies use web scraped data to enhance their operations, informing executive decisions all the way down to individual customer service experiences. \n",
    "\n",
    "### The basics of web scraping\n",
    "It’s extremely simple, in truth, and works by way of two parts: a web crawler and a web scraper. The web crawler is the horse, and the scraper is the chariot. The crawler leads the scraper, as if by the hand, through the internet, where it extracts the data requested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawling\n",
    "\n",
    "Website Crawling is the automated fetching of web pages by a software process, the purpose of which is to index the content of websites so they can be searched. The crawler analyzes the content of a page looking for links to the next pages to fetch and index.\n",
    "\n",
    "##### Types of Crawls\n",
    "There are mainly two types of crawling that get content from the web\n",
    "\n",
    "###### Site Crawl\n",
    "\n",
    "> is an attempt to crawl an entire site at one time, starting with the home page. It will grab links from that page, to continue crawling the site to other content of the site. This is often called “**Spidering**”.\n",
    "\n",
    "###### Page Crawl\n",
    "\n",
    "> which is an attempt by a crawler to crawl a single page or blog post for the content retrieval\n",
    "\n",
    "\n",
    "##### Crawler\n",
    "> A crawler is a software process that goes out to websites and requests the content as a browser would. After that, an indexing process actually picks out the content it wants to save. Typically the content that is indexed is any text visible on the page.\n",
    "\n",
    "##### /robots.txt\n",
    "\n",
    "    1. The site owner denies indexing and or crawling using a robots.txt file.\n",
    "    2. The page itself may indicate it’s not to be indexed and links not followed (directives embedded in the page code). These directives are “meta” tags that tell the crawler how it is allowed to interact with the site.\n",
    "    3. The site owner blocked a specific crawler IP address or “user agent”.\n",
    "For more details on the robots.txt please refer this link[http://www.robotstxt.org/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### installables\n",
    "> pip install webdriver-manager [https://pypi.org/project/webdriver-manager/]\n",
    "\n",
    "__or__\n",
    "\n",
    "> conda install webdriver-manager\n",
    "\n",
    "> pip install selenium\n",
    "\n",
    "__or__\n",
    "\n",
    "> conda install -c conda-forge selenium\n",
    "\n",
    "> pip install bs4\n",
    "\n",
    "__or__\n",
    "\n",
    "> conda install -c anaconda beautifulsoup4\n",
    "\n",
    "> pip install urllib3\n",
    "\n",
    "__or__\n",
    "\n",
    "> conda install urllib3\n",
    "\n",
    "\n",
    "> pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source https://docs.python.org/3/library/concurrent.futures.html\n",
    "import concurrent.futures \n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "import enum\n",
    "import json\n",
    "import time \n",
    "\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys   # for necessary browser action\n",
    "from selenium.webdriver.common.by import By    # For selecting html code\n",
    "\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from scipy.stats import mode\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import seaborn as sb\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "sb.set(style=\"darkgrid\")\n",
    "# Defining the display of generated image in the centre of the page.\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "#Scalar fit Transformation Class [note: will try using the mean and std for normalization]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "'''\n",
    "    HeadLess Options for the Chrome\n",
    "'''\n",
    "def get_chrome_options():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"disable-infobars\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return options\n",
    "\n",
    "\n",
    "def get_color():\n",
    "    rgd = np.random.rand(3, )\n",
    "    # if self._verbose:\n",
    "    #    print('Obtained Color RGD[{}]'.format(rgd))\n",
    "    return rgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a Site URL to crawl and get the necessary indexes for scraping\n",
    "# - Not Implemented as part of this excercise\n",
    "class Crawler:\n",
    "    \n",
    "    def __init__(self, url, depth):\n",
    "        self.depth = depth\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Crawler (Travel Depth : {})\".format(self.depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping\n",
    "A web scraper is a specialized tool designed to accurately and quickly extract data from a web page. Web scrapers vary widely in design and complexity, depending on the project.\n",
    "\n",
    "The web scraping process: **3 simple steps**\n",
    "\n",
    "1. First, our team of seasoned scraping veterans develops a scraper unique to your project, designed specifically to target and extract the data you want from the websites you want it from.\n",
    "2.\tThe data is retrieved in HTML format, after which it is carefully parsed to extricate the raw data you want from the noise surrounding it. Depending on the project, the data can be as simple as a name and address in some cases, and as complex as high dimensional weather and seed germination data the next.\n",
    "3.\tUltimately, the data is stored in the format and to the exact specifications of the project. Some companies use third party applications or databases to view and manipulate the data to their choosing, while others prefer it in a simple, raw format - generally as CSV, TSV or JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targetted Enumeration for the Project\n",
    "\n",
    "class FMCG_Category(enum.Enum):\n",
    "    VEGETABLES = 1\n",
    "    BEVERAGES = 2\n",
    "    STAPLES = 3\n",
    "    SNACKS = 4\n",
    "    DAIRY = 5\n",
    "    MEATS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapper Class with multi threaded processing support\n",
    "\n",
    "\n",
    "class WebElementScrapper:\n",
    "\n",
    "    __author__ = 'Artificial Monks'\n",
    "    '''\n",
    "        Enables the Scrape operation with the help of selenium and json/rest calls to the middleware of the plaforms\n",
    "\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 driver_name,\n",
    "                 thread_pool_size,\n",
    "                 infinity_scroll=False,\n",
    "                 scroll_length=4,\n",
    "                 page_wait=2):\n",
    "        if driver_name != None:\n",
    "            ''' let the system know the driver location'''\n",
    "            self.driver_name = driver_name\n",
    "\n",
    "        # Having more resourceful threads will let you scrape earlier\n",
    "        if thread_pool_size > 0 and thread_pool_size < 20:\n",
    "            self.executor = concurrent.futures.ThreadPoolExecutor(\n",
    "                max_workers=thread_pool_size)\n",
    "\n",
    "        self.infinity_scroll = infinity_scroll\n",
    "        self.scroll_length = scroll_length\n",
    "        self.page_wait = page_wait\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"WebElementScrapper (version 1.0) with ThreadPoolExector({})\".format(\n",
    "            self.executor.max_workers)\n",
    "\n",
    "    '''\n",
    "        Method to read the selector from the root element.\n",
    "    '''\n",
    "    def get_value(self, field_element, selector, is_text, attrib):\n",
    "        ret_val = ''\n",
    "        try:\n",
    "            # If the element is having value in the as text then is_text is required to be True\n",
    "            # else this function will try target the attribute value\n",
    "            if isinstance(field_element, WebElement):\n",
    "                selection = field_element.find_element_by_css_selector(\n",
    "                    selector)\n",
    "                ret_val = selection.text if is_text else selection.get_attribute(\n",
    "                    attrib)\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "        return ret_val\n",
    "\n",
    "    '''\n",
    "        Helps in scraping individual root element with the help of meta_data provided\n",
    "    '''\n",
    "    def scrape_individual_root(self, each_root_element, root_element_metadata):\n",
    "        product_rel_dict = {}\n",
    "        try:\n",
    "            #Each field is designed to have a selector from the root xpath\n",
    "            # and an attribute to capture or text if no attribute provided\n",
    "            for field in root_element_metadata:\n",
    "                field_meta_data = root_element_metadata[field]\n",
    "\n",
    "                #Each field will be located from the root element traversal\n",
    "                product_rel_dict[field] = self.get_value(\n",
    "                    each_root_element, field_meta_data['selector'],\n",
    "                    field_meta_data['text'], field_meta_data['attrib'])\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "        return product_rel_dict\n",
    "\n",
    "    '''\n",
    "    Page will wait for (self.scroll_length * self.page_wait) Seconds, so there will be a good level of performance impact\n",
    "    Utilizes the method window.scrollTo to the max height of the document body so that the scroll event of \n",
    "    Page gets called.\n",
    "    '''\n",
    "    def scroll_page(self, driver):\n",
    "        for i in range(0, self.scroll_length):\n",
    "            driver.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(self.page_wait)\n",
    "\n",
    "    '''\n",
    "        With the given root meta_data scraping will iterate through \n",
    "        the dictionary and copies the information to a dataframe with the aligned category\n",
    "    '''\n",
    "    def scrape_and_get_content(self, driver, url, meta_data_dict):\n",
    "        bag = pd.DataFrame()\n",
    "        # it is okay to return empty dataframe rather than just dying.\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            #If Scroll is enabled the page will scroll to the length that has been mentioned\n",
    "            if self.infinity_scroll == True:\n",
    "                self.scroll_page(driver)\n",
    "\n",
    "            content = driver.page_source\n",
    "            soup = BeautifulSoup(content)\n",
    "            for meta_data_element in meta_data_dict:\n",
    "                # identifies each product root element with the metadata provided\n",
    "                # Need to implement the WebDriverWait(driver, 10)\n",
    "                #.until(EC.presence_of_element_located((By.ID, \"myDynamicElement\")))\n",
    "                meta_elements = driver.find_elements_by_css_selector(\n",
    "                    meta_data_element)\n",
    "                # for each meta element get dictionary\n",
    "                for each_root_element in meta_elements:\n",
    "                    product_rel_dict = self.scrape_individual_root(\n",
    "                        each_root_element, meta_data_dict[meta_data_element])\n",
    "                    if product_rel_dict != None:\n",
    "                        bag = bag.append(pd.Series(data=product_rel_dict),\n",
    "                                         ignore_index=True)\n",
    "        finally:\n",
    "            driver.quit()\n",
    "        # Return the Collected Bag of Vegetables/Snacks/Beverages and etc.\n",
    "        return bag\n",
    "\n",
    "    '''\n",
    "    driver_location - location of the chrome driver\n",
    "    urls            - is a dictionary where Category and URL must be provided\n",
    "                      as the Dataframe will be populated with a column about the category information\n",
    "    meta_data       - is a dictionary which will illustrates the root element and the selectors to use\n",
    "                      for further content retrieval\n",
    "    \n",
    "    \n",
    "    A Merged DataFrame will be returned if there are matches else the method will return an empty Dataframe\n",
    "    \n",
    "    Category is the value from the Enumeration FMCG_Category.value\n",
    "    '''\n",
    "    def get_data_frame(self, urls, meta_data):\n",
    "        df = pd.DataFrame()\n",
    "        #Source https://docs.python.org/3/library/concurrent.futures.html\n",
    "\n",
    "        # Headless is mainly used to let the chrome work as a background process.\n",
    "        chrome_options = get_chrome_options()\n",
    "        '''\n",
    "        Initiate new Driver for each thread to allow no-overlapping of the content\n",
    "        '''\n",
    "        future_of_target = {\n",
    "            self.executor.submit(\n",
    "                self.scrape_and_get_content,\n",
    "                webdriver.Chrome(self.driver_name, options=chrome_options),\n",
    "                urls[url], meta_data): url\n",
    "            for url in urls\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_of_target):\n",
    "            category = future_of_target[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                if len(data) > 0:\n",
    "                    data['Category'] = category.value\n",
    "                    #Contact both the frames\n",
    "                    df = pd.concat([df, data])\n",
    "            except Exception as exc:\n",
    "                print('%r generated an exception: %s' % (category, exc))\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Requesting via 'urllib.request' resulted in 403 after observing the robots.txt \n",
    "    implemented the class to use 'selenium'\n",
    "'''\n",
    "\n",
    "\n",
    "class XHRInterceptor:\n",
    "\n",
    "    __author__ = 'Artificial Monks'\n",
    "    '''\n",
    "        Enables the Scrape operation with the help of selenium and json/rest calls to the middleware of the plaforms\n",
    "\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 driver_name,\n",
    "                 thread_pool_size,\n",
    "                 page_scroll=False,\n",
    "                 scroll_limit=2):\n",
    "        if driver_name != None:\n",
    "            ''' let the system know the driver location'''\n",
    "            self.driver_name = driver_name\n",
    "\n",
    "        # Having more resourceful threads will let you scrape earlier\n",
    "        if thread_pool_size > 0 and thread_pool_size < 20:\n",
    "            self.executor = concurrent.futures.ThreadPoolExecutor(\n",
    "                max_workers=thread_pool_size)\n",
    "\n",
    "        self.page_scroll = page_scroll\n",
    "        self.scroll_limit = scroll_limit\n",
    "\n",
    "    '''\n",
    "        Reads the URL and converts the content into json\n",
    "    '''\n",
    "    def read_json(self, url):\n",
    "\n",
    "        # Headless is mainly used to let the chrome work as a background process.\n",
    "        chrome_options = get_chrome_options()\n",
    "\n",
    "        driver = webdriver.Chrome(self.driver_name, options=chrome_options)\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            elem = driver.find_element_by_tag_name(\"pre\")\n",
    "            content = elem.text\n",
    "            json_response = json.loads(content)\n",
    "            return json_response\n",
    "        except Exception as exc:\n",
    "            print('%r generated an exception: %s' % (url, exc))\n",
    "\n",
    "        # Return Empty if the response is broke or any exception occurs\n",
    "        return None\n",
    "\n",
    "    '''\n",
    "    Recursive JSON element generator\n",
    "    1. Reads the entire json and iterates over the keys of the json to match the root element \n",
    "        i.e product element\n",
    "    2. Each Element further read as dict and process only the \n",
    "    '''\n",
    "    def json_element_generator(self, json_input, lookup_key):\n",
    "        if isinstance(json_input, dict):\n",
    "            for k, v in json_input.items():\n",
    "                if k == lookup_key:\n",
    "                    yield v\n",
    "                else:\n",
    "                    yield from self.json_element_generator(v, lookup_key)\n",
    "        elif isinstance(json_input, list):\n",
    "            for item in json_input:\n",
    "                yield from self.json_element_generator(item, lookup_key)\n",
    "\n",
    "    '''\n",
    "        Reads the URL and converts the content and retrieve the values based on the metadata\n",
    "    '''\n",
    "    def get_data_frame(self, url, json_meta_data):\n",
    "        df = pd.DataFrame()\n",
    "        if url != None:\n",
    "            json_input = self.read_json(url)\n",
    "            for meta_data_element in json_meta_data:\n",
    "                # Generator will yield each element that\n",
    "                for split_meta_ele_key in meta_data_element.split('^'):\n",
    "                    for y_item in self.json_element_generator(\n",
    "                            json_input, split_meta_ele_key):\n",
    "                        if isinstance(y_item, list):\n",
    "                            for each_product in y_item:\n",
    "                                product_rel_dict = self.get_product_info(\n",
    "                                    each_product,\n",
    "                                    json_meta_data[meta_data_element])\n",
    "                                if product_rel_dict != None:\n",
    "                                    df = df.append(\n",
    "                                        pd.Series(data=product_rel_dict),\n",
    "                                        ignore_index=True)\n",
    "                        else:\n",
    "                            self.get_product_info(\n",
    "                                y_item, json_meta_data[meta_data_element])\n",
    "\n",
    "        else:\n",
    "            print('Unable to Process the URL {}'.format(url))\n",
    "        # Return the Added or Empty Bag\n",
    "        return df\n",
    "\n",
    "    '''\n",
    "        Enables the Scrolling in case of XHR requests\n",
    "    '''\n",
    "    def get_data_by_scrolling(self, url, json_meta_data):\n",
    "        df_overall = pd.DataFrame()\n",
    "        for i in range(1, self.scroll_limit):\n",
    "            url = url.replace('&page=1&', (re.sub('\\d', '{}'.format(\n",
    "                (i + 1)), '&page=1&')))\n",
    "            #print('Fetching Data for the url {}'.format(url))\n",
    "            df_interim = self.get_data_frame(url, json_meta_data)\n",
    "            #print(df_interim)\n",
    "            if (len(df_interim) > 0):\n",
    "                df_overall = pd.concat([df_overall, df_interim])\n",
    "        return df_overall\n",
    "\n",
    "    '''\n",
    "    Reads a Json|Dictionary of the product info and returns Req version of the same.\n",
    "    '''\n",
    "    def get_product_info(self, item_dict, item_meta_data):\n",
    "        product_rel_dict = {}\n",
    "        try:\n",
    "            #Each field is designed to have a selector from the root xpath\n",
    "            # and an attribute to capture or text if no attribute provided\n",
    "            for field in item_meta_data:\n",
    "                field_meta_data = item_meta_data[field]\n",
    "                #print(field, field_meta_data)\n",
    "                #Each field will be located from the root element traversal\n",
    "                product_rel_dict[field] = item_dict[\n",
    "                    field_meta_data['selector']]\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "        return product_rel_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targetted Platforms\n",
    "\n",
    "1. Grofers\n",
    "2. BigBasket\n",
    "3. Natures Basket\n",
    "\n",
    "We are going to perform two different scraping methods to extract the data from them based on the observation and inspections we performed on the web platforms\n",
    "\n",
    "1. Scraping using the Selenium to read the page source and extract the information\n",
    "2. Intercept the middleware class by the platform to popluate the UI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grofers  - Scraping using WebElementScrapper with Selenium\n",
    "\n",
    "We have prepared a meta data json/dictionay for this scraping where we will guide the scraping tool to identify the root element and read the subsequent elements with the help of meta data provided.\n",
    "\n",
    "##### **Below are our observations:**\n",
    "\n",
    "1. Each product in '**Grofer**' web is wrapped in an anchor with the following info\n",
    "    + Name {div.plp-product__name > title}\n",
    "    + Quantity {div.plp-product__quantity > title}\n",
    "    + Discount {div.plp-product__offer > innerText}\n",
    "    + Membership Discount{div.plp-product__non-member-price > innerText}\n",
    "    + Price {span.plp-product__price--new > innerText}\n",
    "    \n",
    "##### __Following is the metadata utlized in scraping grofers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Retrieval Dictionary\n",
    "# root{ elementkey : [selector, attribute]}\n",
    "\n",
    "grofers_meta_data_dict = {\n",
    "    'a.product__wrapper': {\n",
    "        'Name': {\n",
    "            'selector': 'div.plp-product__name',\n",
    "            'text': False,\n",
    "            'attrib': 'title'\n",
    "        },\n",
    "        'Quantity': {\n",
    "            'selector': 'div.plp-product__quantity',\n",
    "            'text': False,\n",
    "            'attrib': 'title'\n",
    "        },\n",
    "        'Price': {\n",
    "            'selector': 'span.plp-product__price--new',\n",
    "            'text': False,\n",
    "            'attrib': 'innerText'\n",
    "        },\n",
    "        'Discount': {\n",
    "            'selector': 'div.plp-product__offer',\n",
    "            'text': False,\n",
    "            'attrib': 'innerText'\n",
    "        },\n",
    "        'Membership-price': {\n",
    "            'selector': 'div.plp-product__non-member-price',\n",
    "            'text': False,\n",
    "            'attrib': 'innerText'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FMCG** of **Grofers** platform can be scrapped with the following urls for the targetted Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Groffer Scrappable URLS for the FMCG\n",
    "'''\n",
    "\n",
    "GROFER_URLS = {\n",
    "    FMCG_Category.VEGETABLES:\n",
    "    'https://grofers.com/cn/vegetables-fruits/cid/1487',\n",
    "    FMCG_Category.BEVERAGES: 'https://grofers.com/cn/beverages/cid/12',\n",
    "    FMCG_Category.STAPLES: 'https://grofers.com/cn/grocery-staples/cid/16',\n",
    "    FMCG_Category.SNACKS:\n",
    "    'https://grofers.com/cn/biscuits-snacks-chocolates/cid/13',\n",
    "    FMCG_Category.DAIRY: 'https://grofers.com/cn/breakfast-dairy/cid/14'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Initialize the Scraper with driver and thread count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location where i have placed the chromedriver : '/users/shraddha/chromed/chromedriver'\n",
    "scraper_for_grofers = WebElementScrapper(\n",
    "    '/users/shraddha/chromed/chromedriver', 10, True, 4, 1)\n",
    "\n",
    "grofers_df = scraper_for_grofers.get_data_frame(GROFER_URLS,\n",
    "                                                grofers_meta_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Discount</th>\n",
       "      <th>Membership-price</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24% OFF</td>\n",
       "      <td>None</td>\n",
       "      <td>Onion</td>\n",
       "      <td>₹22</td>\n",
       "      <td>1  kg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25% OFF</td>\n",
       "      <td>None</td>\n",
       "      <td>Tomato Hybrid</td>\n",
       "      <td>₹15</td>\n",
       "      <td>1  kg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23% OFF</td>\n",
       "      <td>Club Price: ₹76</td>\n",
       "      <td>Kiwi</td>\n",
       "      <td>₹78</td>\n",
       "      <td>3 units</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23% OFF</td>\n",
       "      <td>Club Price: ₹28</td>\n",
       "      <td>Lemon</td>\n",
       "      <td>₹29</td>\n",
       "      <td>6 unit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24% OFF</td>\n",
       "      <td>Club Price: ₹36</td>\n",
       "      <td>Garlic (Lehsun)</td>\n",
       "      <td>₹37</td>\n",
       "      <td>200 g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>8% OFF</td>\n",
       "      <td>Club Price: ₹115</td>\n",
       "      <td>India Gate Tibar Basmati Rice</td>\n",
       "      <td>₹119</td>\n",
       "      <td>1 kg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>6% OFF</td>\n",
       "      <td>Club Price: ₹765</td>\n",
       "      <td>Saffola Gold Pro Healthy Lifestyle Blended Coo...</td>\n",
       "      <td>₹773</td>\n",
       "      <td>5 l</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>15% OFF</td>\n",
       "      <td>Club Price: ₹102</td>\n",
       "      <td>Shasha Garam Masala Whole (Pouch)</td>\n",
       "      <td>₹106</td>\n",
       "      <td>100 g</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>None</td>\n",
       "      <td>Club Price: ₹368</td>\n",
       "      <td>Parachute Pure Coconut Oil</td>\n",
       "      <td>₹380</td>\n",
       "      <td>1 l</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>46% OFF</td>\n",
       "      <td>Club Price: ₹212</td>\n",
       "      <td>Borges Refined Canola Oil (Bottle)</td>\n",
       "      <td>₹215</td>\n",
       "      <td>1 l</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>951 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Discount  Membership-price  \\\n",
       "0    24% OFF              None   \n",
       "1    25% OFF              None   \n",
       "2    23% OFF   Club Price: ₹76   \n",
       "3    23% OFF   Club Price: ₹28   \n",
       "4    24% OFF   Club Price: ₹36   \n",
       "..       ...               ...   \n",
       "235   8% OFF  Club Price: ₹115   \n",
       "236   6% OFF  Club Price: ₹765   \n",
       "237  15% OFF  Club Price: ₹102   \n",
       "238     None  Club Price: ₹368   \n",
       "239  46% OFF  Club Price: ₹212   \n",
       "\n",
       "                                                  Name Price Quantity  \\\n",
       "0                                                Onion   ₹22    1  kg   \n",
       "1                                        Tomato Hybrid   ₹15    1  kg   \n",
       "2                                                 Kiwi   ₹78  3 units   \n",
       "3                                                Lemon   ₹29   6 unit   \n",
       "4                                      Garlic (Lehsun)   ₹37    200 g   \n",
       "..                                                 ...   ...      ...   \n",
       "235                      India Gate Tibar Basmati Rice  ₹119     1 kg   \n",
       "236  Saffola Gold Pro Healthy Lifestyle Blended Coo...  ₹773      5 l   \n",
       "237                  Shasha Garam Masala Whole (Pouch)  ₹106    100 g   \n",
       "238                         Parachute Pure Coconut Oil  ₹380      1 l   \n",
       "239                 Borges Refined Canola Oil (Bottle)  ₹215      1 l   \n",
       "\n",
       "     Category  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "..        ...  \n",
       "235         3  \n",
       "236         3  \n",
       "237         3  \n",
       "238         3  \n",
       "239         3  \n",
       "\n",
       "[951 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grofers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BigBasket  - Scraping using XHRIntercept with Selenium\n",
    "\n",
    "Big Basket webpage is constructed with the help of **xml+html**, so instead scraping the page we looked at the **NetWork** for any information that we can capture.\n",
    "\n",
    "##### **Below are our observations:**\n",
    "1. Page content is loaded with the help of __/product/get-products/__ api call with parameters as\n",
    "    + **slug**=fruits-vegetables|beverages\n",
    "    + **tab_type**=[\"all\"] - Seems related to the tabs that are being loaded on the left handside of the page or top of the page\n",
    "    + Observed URLs for our targetted category\n",
    "        - Fruits and Vegetables[https://www.bigbasket.com/product/get-products/?slug=beverages&page=2&tab_type=[%22all%22]&sorted_on=popularity&listtype=pc]\n",
    "        - Beverages [https://www.bigbasket.com/product/get-products/?slug=fruits-vegetables&page=2&tab_type=[]&sorted_on=popularity&listtype=pc]\n",
    "        - Meats [https://www.bigbasket.com/product/get-products/?slug=eggs-meat-fish&page=2&tab_type=[%22all%22]&sorted_on=popularity&listtype=pc]\n",
    "        - Staples [https://www.bigbasket.com/product/get-products/?slug=foodgrains-oil-masala&page=2&tab_type=[%22all%22]&sorted_on=popularity&listtype=pc]\n",
    "        etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL Dictionary for the BigBasket\n",
    "BB_URLS = {\n",
    "    FMCG_Category.VEGETABLES:\n",
    "    'https://www.bigbasket.com/product/get-products/?slug=beverages&page=1&tab_type=[%22all%22]&sorted_on=popularity&listtype=pc',\n",
    "    FMCG_Category.MEATS:\n",
    "    'https://www.bigbasket.com/product/get-products/?slug=eggs-meat-fish&page=1&tab_type=[%22all%22]&sorted_on=popularity&listtype=pc',\n",
    "    FMCG_Category.BEVERAGES:\n",
    "    'https://www.bigbasket.com/product/get-products/?slug=fruits-vegetables&page=1&tab_type=[]&sorted_on=popularity&listtype=pc',\n",
    "    FMCG_Category.DAIRY:\n",
    "    'https://www.bigbasket.com/product/get-products/?slug=bakery-cakes-dairy&page=1&tab_type=[%22all%22]&sorted_on=popularity&listtype=pc',\n",
    "    FMCG_Category.STAPLES:\n",
    "    'https://www.bigbasket.com/product/get-products/?slug=foodgrains-oil-masala&page=1&tab_type=[%22all%22]&sorted_on=popularity&listtype=pc',\n",
    "    FMCG_Category.SNACKS:\n",
    "    'https://www.bigbasket.com/product/get-products/?slug=snacks-branded-foods&page=1&tab_type=[%22all%22]&sorted_on=popularity&listtype=pc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "interceptor = XHRInterceptor('/users/shraddha/chromed/chromedriver', 10, True,\n",
    "                             5)\n",
    "#when the website has two different root elements tagged for the products to scrape\n",
    "bb_metadata = {\n",
    "    'prods^products': {\n",
    "        'Name': {\n",
    "            'selector': 'llc_n',\n",
    "            'text': False,\n",
    "            'attrib': 'title'\n",
    "        },\n",
    "        'Quantity': {\n",
    "            'selector': 'w',\n",
    "            'text': False,\n",
    "            'attrib': 'title'\n",
    "        },\n",
    "        'Price': {\n",
    "            'selector': 'mrp',\n",
    "            'text': False,\n",
    "            'attrib': 'innerText'\n",
    "        },\n",
    "        'Discount Price': {\n",
    "            'selector': 'sp',\n",
    "            'text': False,\n",
    "            'attrib': 'innerText'\n",
    "        },\n",
    "        'Product Type': {\n",
    "            'selector': 'p_type',\n",
    "            'text': False,\n",
    "            'attrib': 'innerText'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "bb_df = pd.DataFrame()\n",
    "# We figured scrolling is happending on the xhr json requests with the\n",
    "# help of &page=1& so we have replaced appropriately\n",
    "\n",
    "for url in BB_URLS:\n",
    "    try:\n",
    "        data = interceptor.get_data_by_scrolling(BB_URLS[url], bb_metadata)\n",
    "        if len(data) > 0:\n",
    "            data['Category'] = url.value\n",
    "            #Contact both the frames\n",
    "            bb_df = pd.concat([bb_df, data])\n",
    "    except Exception as exc:\n",
    "        print('%r generated an exception: %s' % (category, exc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Product Type</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.00</td>\n",
       "      <td>Instant Coffee</td>\n",
       "      <td>10.00</td>\n",
       "      <td></td>\n",
       "      <td>11 g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.01</td>\n",
       "      <td>Cold Drinks</td>\n",
       "      <td>150.00</td>\n",
       "      <td></td>\n",
       "      <td>6x250 ml</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.00</td>\n",
       "      <td>Flavoured, Soya Milk</td>\n",
       "      <td>100.00</td>\n",
       "      <td></td>\n",
       "      <td>4x200 ml</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61.60</td>\n",
       "      <td>Cold Drinks</td>\n",
       "      <td>70.00</td>\n",
       "      <td></td>\n",
       "      <td>2x600 ml</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.00</td>\n",
       "      <td>Juices</td>\n",
       "      <td>10.00</td>\n",
       "      <td></td>\n",
       "      <td>160 ml</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20.00</td>\n",
       "      <td>Chocolates</td>\n",
       "      <td>20.00</td>\n",
       "      <td></td>\n",
       "      <td>23 g</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>255.75</td>\n",
       "      <td>Frozen Veg Snacks</td>\n",
       "      <td>275.00</td>\n",
       "      <td>veg</td>\n",
       "      <td>1.25 kg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>337.50</td>\n",
       "      <td>Frozen Non-Veg Snacks</td>\n",
       "      <td>375.00</td>\n",
       "      <td>non-veg</td>\n",
       "      <td>750 g</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>160.00</td>\n",
       "      <td>Cookies</td>\n",
       "      <td>160.00</td>\n",
       "      <td></td>\n",
       "      <td>400 g</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>575.00</td>\n",
       "      <td>Muesli</td>\n",
       "      <td>575.00</td>\n",
       "      <td>veg</td>\n",
       "      <td>1 kg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Discount Price                   Name   Price Product Type  Quantity  \\\n",
       "0           10.00         Instant Coffee   10.00                   11 g   \n",
       "1          125.01            Cold Drinks  150.00               6x250 ml   \n",
       "2           90.00   Flavoured, Soya Milk  100.00               4x200 ml   \n",
       "3           61.60            Cold Drinks   70.00               2x600 ml   \n",
       "4           10.00                 Juices   10.00                 160 ml   \n",
       "..            ...                    ...     ...          ...       ...   \n",
       "15          20.00             Chocolates   20.00                   23 g   \n",
       "16         255.75      Frozen Veg Snacks  275.00          veg   1.25 kg   \n",
       "17         337.50  Frozen Non-Veg Snacks  375.00      non-veg     750 g   \n",
       "18         160.00                Cookies  160.00                  400 g   \n",
       "19         575.00                 Muesli  575.00          veg      1 kg   \n",
       "\n",
       "    Category  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "..       ...  \n",
       "15         4  \n",
       "16         4  \n",
       "17         4  \n",
       "18         4  \n",
       "19         4  \n",
       "\n",
       "[400 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natures Basket - Scraping using WebElementScrapper with Selenium\n",
    "\n",
    "Nature Basket has similar way to the Grofers where the html is being returned from the web request, hence we chose to process the Scraping mechanism with the help of **WebElementScrapper**\n",
    "\n",
    "\n",
    "##### **Below are our observations:**\n",
    "\n",
    "1. Each product in '**Nature's Basket**' web is wrapped in an __<div />__ with the following info\n",
    "    + Name {a.search_Ptitle}\n",
    "    + Quantity {span.search_PSelectedSize}\n",
    "    + Price {div.productlist-price}\n",
    "    + Availability {span.hrsdelivery} - Some of the products are mentioned as **Limited**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Dictionary\n",
    "\n",
    "# Product Retrieval Dictionary\n",
    "# root{ elementkey : [selector, attribute]}\n",
    "\n",
    "nb_meta_data_dict = {\n",
    "    'div.Search': {\n",
    "        'Name': {\n",
    "            'selector': 'a.search_Ptitle',\n",
    "            'text': False,\n",
    "            'attrib': 'innerText'\n",
    "        },\n",
    "        'Quantity': {\n",
    "            'selector': 'span.search_PSelectedSize',\n",
    "            'text': False,\n",
    "            'attrib': 'innerText'\n",
    "        },\n",
    "        'Price': {\n",
    "            'selector': 'div.productlist-price',\n",
    "            'text': False,\n",
    "            'attrib': 'innerText'\n",
    "        },\n",
    "        'Availability': {\n",
    "            'selector': 'span.hrsdelivery',\n",
    "            'text': False,\n",
    "            'attrib': 'innerText'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Natures Basket Scrappable URLS for the FMCG\n",
    "    Scroll URL, Throws internal error if the invocation happens outside, \n",
    "    hence used the scroll_page of the scrapper\n",
    "    https://www.naturesbasket.co.in/Handlers/SearchHandler.ashx?GetMoreSearchData=True&Keyword=&SupercategoryID=5&CategoryID=0&SubCategoryID=0&BrandID=&AttributeValueID=0&fromPrice=0&toPrice=400000&fromSize=1&toSize=1000&UnitID=1&fromCount=61&SortID=1&DietaryId=&IsOutOfStock=&IsNewProduct=\n",
    "'''\n",
    "\n",
    "NB_URLS = {\n",
    "    FMCG_Category.VEGETABLES:\n",
    "    'https://www.naturesbasket.co.in/Online-grocery-shopping/Fruits---Vegetables/5_0_0',\n",
    "    FMCG_Category.MEATS:\n",
    "    'https://www.naturesbasket.co.in/Online-grocery-shopping/Meats-Seafood-and-Eggs/8_0_0',\n",
    "    FMCG_Category.BEVERAGES:\n",
    "    'https://www.naturesbasket.co.in/Online-grocery-shopping/Snacks---Beverages/9_0_0',\n",
    "    FMCG_Category.DAIRY:\n",
    "    'https://www.naturesbasket.co.in/Online-grocery-shopping/Breakfast--Dairy---Bakery/11_0_0',\n",
    "    FMCG_Category.STAPLES:\n",
    "    'https://www.naturesbasket.co.in/Online-grocery-shopping/Indian-Grocery/12_0_0',\n",
    "    FMCG_Category.SNACKS: 'https://www.naturesbasket.co.in/Online-grocery-shopping/Health/3_0_0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location where i have placed the chromedriver : '/users/shraddha/chromed/chromedriver'\n",
    "scraper_for_nb = WebElementScrapper('/users/shraddha/chromed/chromedriver', 10, False, 2, 1)\n",
    "\n",
    "nb_df = scraper_for_nb.get_data_frame(NB_URLS, nb_meta_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Availability</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIMITED AVAILABILITY</td>\n",
       "      <td>Dev Alphonso Mango Regular</td>\n",
       "      <td>MRP ₹59</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIMITED AVAILABILITY</td>\n",
       "      <td>Mango Raw</td>\n",
       "      <td>MRP ₹32.25</td>\n",
       "      <td>250 g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>Mango Lalbag - Natures Basket</td>\n",
       "      <td>MRP ₹54.75</td>\n",
       "      <td>250 g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Kesar Mango - Fruit &amp; Vegetable</td>\n",
       "      <td>MRP ₹149.5</td>\n",
       "      <td>500 g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Mango Totapuri</td>\n",
       "      <td>MRP ₹74.5</td>\n",
       "      <td>500 g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>None</td>\n",
       "      <td>Kolam Rice - Nature's</td>\n",
       "      <td>MRP ₹89</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>None</td>\n",
       "      <td>Moong Whole - Nature's</td>\n",
       "      <td>MRP ₹109</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>None</td>\n",
       "      <td>Kerala Red Rice - Nature's</td>\n",
       "      <td>MRP ₹89</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>None</td>\n",
       "      <td>Chana Green - Nature's</td>\n",
       "      <td>MRP ₹79</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>None</td>\n",
       "      <td>Urad Dal - Dhuli - Nature's</td>\n",
       "      <td>MRP ₹99</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Availability                             Name       Price  \\\n",
       "0   LIMITED AVAILABILITY       Dev Alphonso Mango Regular     MRP ₹59   \n",
       "1   LIMITED AVAILABILITY                        Mango Raw  MRP ₹32.25   \n",
       "2                   None    Mango Lalbag - Natures Basket  MRP ₹54.75   \n",
       "3                   None  Kesar Mango - Fruit & Vegetable  MRP ₹149.5   \n",
       "4                   None                   Mango Totapuri   MRP ₹74.5   \n",
       "..                   ...                              ...         ...   \n",
       "25                  None            Kolam Rice - Nature's     MRP ₹89   \n",
       "26                  None           Moong Whole - Nature's    MRP ₹109   \n",
       "27                  None       Kerala Red Rice - Nature's     MRP ₹89   \n",
       "28                  None           Chana Green - Nature's     MRP ₹79   \n",
       "29                  None      Urad Dal - Dhuli - Nature's     MRP ₹99   \n",
       "\n",
       "   Quantity  Category  \n",
       "0      None         1  \n",
       "1     250 g         1  \n",
       "2     250 g         1  \n",
       "3     500 g         1  \n",
       "4     500 g         1  \n",
       "..      ...       ...  \n",
       "25     None         4  \n",
       "26     None         4  \n",
       "27     None         4  \n",
       "28     None         4  \n",
       "29     None         4  \n",
       "\n",
       "[180 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning / Data Preperation\n",
    "\n",
    "Most of the data that we have observed during the scraping there is a need to clean data so that we can perform further calculations. Below are examples of such observations in the data captured.\n",
    "\n",
    "> Grofers' Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Discount</th>\n",
       "      <th>Membership-price</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24% OFF</td>\n",
       "      <td>None</td>\n",
       "      <td>Onion</td>\n",
       "      <td>₹22</td>\n",
       "      <td>1  kg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25% OFF</td>\n",
       "      <td>None</td>\n",
       "      <td>Tomato Hybrid</td>\n",
       "      <td>₹15</td>\n",
       "      <td>1  kg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23% OFF</td>\n",
       "      <td>Club Price: ₹76</td>\n",
       "      <td>Kiwi</td>\n",
       "      <td>₹78</td>\n",
       "      <td>3 units</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23% OFF</td>\n",
       "      <td>Club Price: ₹28</td>\n",
       "      <td>Lemon</td>\n",
       "      <td>₹29</td>\n",
       "      <td>6 unit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24% OFF</td>\n",
       "      <td>Club Price: ₹36</td>\n",
       "      <td>Garlic (Lehsun)</td>\n",
       "      <td>₹37</td>\n",
       "      <td>200 g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Discount Membership-price             Name Price Quantity  Category\n",
       "0  24% OFF             None            Onion   ₹22    1  kg         1\n",
       "1  25% OFF             None    Tomato Hybrid   ₹15    1  kg         1\n",
       "2  23% OFF  Club Price: ₹76             Kiwi   ₹78  3 units         1\n",
       "3  23% OFF  Club Price: ₹28            Lemon   ₹29   6 unit         1\n",
       "4  24% OFF  Club Price: ₹36  Garlic (Lehsun)   ₹37    200 g         1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grofers_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nature's Basket Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Availability</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIMITED AVAILABILITY</td>\n",
       "      <td>Dev Alphonso Mango Regular</td>\n",
       "      <td>MRP ₹59</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIMITED AVAILABILITY</td>\n",
       "      <td>Mango Raw</td>\n",
       "      <td>MRP ₹32.25</td>\n",
       "      <td>250 g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>Mango Lalbag - Natures Basket</td>\n",
       "      <td>MRP ₹54.75</td>\n",
       "      <td>250 g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Kesar Mango - Fruit &amp; Vegetable</td>\n",
       "      <td>MRP ₹149.5</td>\n",
       "      <td>500 g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Mango Totapuri</td>\n",
       "      <td>MRP ₹74.5</td>\n",
       "      <td>500 g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Availability                             Name       Price Quantity  \\\n",
       "0  LIMITED AVAILABILITY       Dev Alphonso Mango Regular     MRP ₹59     None   \n",
       "1  LIMITED AVAILABILITY                        Mango Raw  MRP ₹32.25    250 g   \n",
       "2                  None    Mango Lalbag - Natures Basket  MRP ₹54.75    250 g   \n",
       "3                  None  Kesar Mango - Fruit & Vegetable  MRP ₹149.5    500 g   \n",
       "4                  None                   Mango Totapuri   MRP ₹74.5    500 g   \n",
       "\n",
       "   Category  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Big Basket Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Discount Price</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Product Type</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.00</td>\n",
       "      <td>Instant Coffee</td>\n",
       "      <td>10.00</td>\n",
       "      <td></td>\n",
       "      <td>11 g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.01</td>\n",
       "      <td>Cold Drinks</td>\n",
       "      <td>150.00</td>\n",
       "      <td></td>\n",
       "      <td>6x250 ml</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.00</td>\n",
       "      <td>Flavoured, Soya Milk</td>\n",
       "      <td>100.00</td>\n",
       "      <td></td>\n",
       "      <td>4x200 ml</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61.60</td>\n",
       "      <td>Cold Drinks</td>\n",
       "      <td>70.00</td>\n",
       "      <td></td>\n",
       "      <td>2x600 ml</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.00</td>\n",
       "      <td>Juices</td>\n",
       "      <td>10.00</td>\n",
       "      <td></td>\n",
       "      <td>160 ml</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Discount Price                  Name   Price Product Type  Quantity  \\\n",
       "0          10.00        Instant Coffee   10.00                   11 g   \n",
       "1         125.01           Cold Drinks  150.00               6x250 ml   \n",
       "2          90.00  Flavoured, Soya Milk  100.00               4x200 ml   \n",
       "3          61.60           Cold Drinks   70.00               2x600 ml   \n",
       "4          10.00                Juices   10.00                 160 ml   \n",
       "\n",
       "   Category  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Data & Duplicate Data\n",
    "\n",
    "1. Prices with additional string literals such as '**MRP**', '**Rs**', '**₹**' along with the numbers\n",
    "3. Discounts with literals being prefixed or suffixed\n",
    "\n",
    "#### Solution\n",
    "##### Duplicate Removal and re-index\n",
    "As the Scraping done using the generic algorithms there may exist duplicate data, so removal of data gives us a better dataset to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(951, 6)\n",
      "(903, 6)\n"
     ]
    }
   ],
   "source": [
    "# Duplicate Removal\n",
    "print(grofers_df.shape)\n",
    "grofers_df = grofers_df.drop_duplicates('Name')\n",
    "print(grofers_df.shape)\n",
    "grofers_df = grofers_df.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 5)\n",
      "(147, 5)\n"
     ]
    }
   ],
   "source": [
    "# Duplicate Removal\n",
    "print(nb_df.shape)\n",
    "nb_df = nb_df.drop_duplicates('Name')\n",
    "print(nb_df.shape)\n",
    "nb_df = nb_df.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 6)\n",
      "(59, 6)\n"
     ]
    }
   ],
   "source": [
    "# Duplicate Removal\n",
    "print(bb_df.shape)\n",
    "bb_df = bb_df.drop_duplicates('Name')\n",
    "print(bb_df.shape)\n",
    "bb_df = bb_df.reindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Noise Clearning\n",
    "###### __Removal of 'MRP ₹' from the Price column in Grofer's & Nature's Basket DataFrames__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Availability     object\n",
       "Name             object\n",
       "Price           float64\n",
       "Quantity         object\n",
       "Category          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_df['Price'] = nb_df['Price'].map(\n",
    "    lambda x: float(re.sub(\"\\D\", \"\", str(x)).strip()) if x != None else 0)\n",
    "\n",
    "nb_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discount             object\n",
       "Membership-price     object\n",
       "Name                 object\n",
       "Price               float64\n",
       "Quantity             object\n",
       "Category              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grofers_df['Price'] = grofers_df['Price'].map(\n",
    "    lambda x: float(re.sub(\"\\D\", \"\", str(x)).strip()) if x != None else 0)\n",
    "\n",
    "grofers_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### __Removal of 'Club Price ' from Membership Discount and %OFF from Discount Column__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discount            float64\n",
       "Membership-price    float64\n",
       "Name                 object\n",
       "Price               float64\n",
       "Quantity             object\n",
       "Category              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grofers_df['Membership-price'] = grofers_df['Membership-price'].map(\n",
    "    lambda x: float(re.sub(\"\\D\", \"\", str(x)).strip()) if x != None else 0)\n",
    "\n",
    "grofers_df['Discount'] = grofers_df['Discount'].map(\n",
    "    lambda x: float(re.sub(\"\\D\", \"\", str(x)).strip()) if x != None else 0)\n",
    "\n",
    "grofers_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorize the missing values into NMAR/MAR/MACR based on the observation**\n",
    "\n",
    "1. Discount value is missing in Nature's Basket, which can be considered/safely assumed as **No Discount** at the moment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation\n",
    "\n",
    "1. Quantity being quoted in vocabulary so need conversion to numerical units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration\n",
    "\n",
    "In-order to perform the integration the dataframe has to have the same column set, but when we looked at different websites we were able to scape a minimal set of data being intersected with respect to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Discount', 'Membership-price', 'Name', 'Price', 'Quantity',\n",
      "       'Category'],\n",
      "      dtype='object')\n",
      "Index(['Availability', 'Name', 'Price', 'Quantity', 'Category'], dtype='object')\n",
      "Index(['Discount Price', 'Name', 'Price', 'Product Type', 'Quantity',\n",
      "       'Category'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Combined all data sets based on the data that has been retrieved\n",
    "print(grofers_df.columns)\n",
    "print(nb_df.columns)\n",
    "print(bb_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA & Recommandation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[https://data-lessons.github.io/library-webscraping-DEPRECATED/]\n",
    "\n",
    "[https://www.techbeamers.com/locate-elements-selenium-python/]\n",
    "\n",
    "[https://pythonexamples.org/pandas-dataframe-add-append-row/]\n",
    "\n",
    "##### What is robots.txt?\n",
    "\n",
    "1. Flipkart[https://www.flipkart.com/robots.txt]\n",
    "\n",
    "2. https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3\n",
    "\n",
    "\n",
    "[https://selenium-python.readthedocs.io/locating-elements.html]\n",
    "Must Read on the 'selenium.webdriver.remote.webelement.WebElement'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
